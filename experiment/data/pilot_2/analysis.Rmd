---
title: "Pilot 2 Analysis"
author: "Josh Rule"
date: "6/23/2020"
output:
  bookdown::html_document2:
    keep_md: true
    fig_caption: true
    toc: true
    toc_float:
      collapsed: true
---

```{r setup, include=FALSE}
library(furrr)
library(ggplot2)
library(Hmisc)
library(latex2exp)
library(lme4)
library(lmerTest)
library(lubridate)
library(patchwork)
library(kableExtra)
library(readr)
library(RColorBrewer)
library(reticulate)
library(tidyboot)
library(tidyverse)
library(zoo)

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(eval.after = "fig.cap")
use_virtualenv("~/.data/.virtualenvs/experiment-_gdhu0nT")
options(dplyr.summarise.inform=F)
```

```{r utilities, include=FALSE}
report_spread <- function(title, xs) {
  ## test for normality with Shapiro-Wilk (~shapiro.test~)
  ## if normal, report mean and variance and test results
  ## else report quartiles and test results
  sw <- shapiro.test(xs)
  qs <- quantile(xs, na.rm=T)
  nna <- sum(is.na(xs))
  ## return median, IQR, and failed normality test
  list(title=title, min=qs[1][1], fst=qs[2], median=qs[3], trd=qs[4], max=qs[5], w=sw$statistic, p=sw$p.value, missing=nna)
}
rank_participants <- function(dt) {
  dt %>%
    group_by(subject) %>%
    dplyr::summarize(k.test=sum(accuracy)) %>%
    mutate(rank=rank(1/k.test, ties.method='random')) %>%
    select(-k.test)
}
last_error <- function(xs) {
  # return the index of the last element to contain a 0
  indices <- which(!as.logical(xs))
  if (length(indices) == 0) {
    0
  } else {
    max(indices)
  }
}
m_of_n_correct <- function(m, n, xs) {
  rollapply(xs, n, sum, align="right", fill=0) >= m
}
first_m_of_n <- function(m, n, xs) {
  if (m == 0) {
    0
  } else {
    indices <- which(m_of_n_correct(m, n ,xs))
    if (length(indices) == 0) {
      NA
    } else {
      min(indices)
    }    
  }
}
first_m <- function(m, xs) {
  if (m == 0) {
    0
  } else {
    indices <- which(as.logical(xs))
    if (length(indices) < m) {
      NA
    } else {
      indices[m]
    }
  }
}
incremental_probability <- function(xs) {
  seq_along(xs) %>% map_dbl(~ sum(xs[.:length(xs)]/(length(xs)-.+1)))
}
cumulative_probability <- function(xs) {
  seq_along(xs) %>% map_dbl(~ sum(xs[1:.]/.))
}
exp_weighted_helper <- function(xs) {
  ws <- rev(1/(2^(seq_along(xs)-1)))
  sum(xs*ws)/sum(ws)
}
exp_weighted_probability <- function(xs) {
  seq_along(xs) %>% map_dbl(~ exp_weighted_helper(xs[1:.]))
}
import_time_data <- function(time_path) {
  read_csv(time_path, col_names = T, na = c("", "NA")) %>%
    rename(subject = uniqueid) %>%
    mutate(block = factor(block, levels=c("instructions", "prequiz", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "postquiz"), labels=c("Instructions", "Quiz", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "Survey")))
}
import_participant_data <- function(participant_path) {
  read_csv(participant_path,
           col_names = T,
           na = c("", "NA", "no-response")) %>%
    select(-starts_with("prequiz")) %>%
    select(-one_of(c("assignmentId", "browser", "mode",
                     "currenttrial", "hitId", "ipadress",
                     "platform", "useragent", "status",
                     "condition", "counterbalance"))) %>%
    rename(
      subject = uniqueid,
      experience = `postquiz-q5`,
      comments = `postquiz-q6`,
      difficulty = `postquiz-q1`,
      engagement = `postquiz-q2`,
      gender = `postquiz-q3`,
      dob = `postquiz-q4`) %>%
    mutate(
      gender = factor(gender),
      dob = as_date(dob),
      dob = replace(dob, dob > ymd("2019-02-05") | dob < ymd("1900-01-01"), NA),
      age = (dob %--% ymd("2020-02-05")) / years(1),
      total_time = (beginhit %--% endhit) / minutes(1))
}
unique_id <- function(purpose, id) {
  str_c(str_sub(purpose, 1, 1), str_sub(id, 2))
}
import_trial_data <- function(trials_path, stimuli_path) {
  read_csv(trials_path, col_names = T) %>%
    select(-phase, -condition, -id) %>%
    rename(subject = uniqueid) %>%
    left_join(read_csv(stimuli_path, col_names = T) %>%
                mutate(id = unique_id(purpose, id)) %>%
                rename(concept = program)) %>%
    mutate(
      id = factor(id),
      block_trial = block_trial + 1,
      log_block_trial = log(block_trial)
    ) %>%
    left_join(rank_participants(.)) %>%
    select(
      subject,
      total_trial,
      block,
      block_trial,
      log_block_trial,
      id,
      accuracy,
      everything()
    )
}
```

```{r dp, include = FALSE}
dp <- import_participant_data("./participants.csv")
```

```{r dm, include = FALSE}
dm <- import_time_data("./times.csv")
```

```{r dt, include = FALSE}
dt <- import_trial_data('./trials.csv', './stimuli.csv')
dtl <- dt %>% filter(task == '[(i,o)]->i->o')
concepts <- read_csv("./concepts.csv") %>%
  mutate(id = unique_id(purpose, id)) %>%
  left_join(
    dtl %>%
      group_by(subject, id) %>%
      dplyr::summarize(learned = !is.na(first_m_of_n(3, 3, accuracy))) %>%
      ungroup() %>%
      group_by(id) %>%
      dplyr::summarize(pct_learned = sum(learned) / n())
  ) %>%
  mutate(name = fct_reorder(factor(name), pct_learned, .desc=T))
dtl <- dtl %>% left_join(concepts)
dtv <-
  dt %>% filter(task == '[(i,o)]->spec') %>% select(
    -total_trial,
    -block_trial,
    -log_block_trial,
    -id,
    -accuracy,
    -purpose,
    -input,
    -output,
    -rt,
    -order
  ) %>% left_join(concepts)
```

# Introduction 

This document analyzes data collected during a 2020-06-22 pilot of `v0.0.5` of the list-routines experiment. The analysis focuses on questions relevant to running a full-scale study.

# Method

## Participants

`r length(unique(dp$subject))` people participated in the experiment, hosted on Amazon Mechanical Turk via PsiTurk. Participant age ranged from `r round(min(dp$age), 1)`yrs to `r round(max(dp$age), 1)`yrs (median: `r round(median(dp$age), 1)`yrs), with `r sum(dp$gender == "male", na.rm=T)` males and `r sum(dp$gender == "female", na.rm=T)` females (Figure \@ref(fig:participants)).

```{r participants, echo=FALSE, warning=FALSE, fig.width=3, fig.height=3, out.width = "24.9%", fig.align='center', fig.show='hold', fig.cap="Histograms of participant age and gender, respectively."}
ggplot(dp, aes(x = age)) +
  geom_histogram(
    binwidth = 5,
    colour = "black",
    fill = "white",
    size = 0.25,
    na.rm = T
  ) +
  xlab("Age (yr)") +
  ylab("Number of Participants")

ggplot(dp %>% mutate(gender = fct_explicit_na(gender, na_level = "No Response")) %>% count(gender),
       aes(x = gender, y = n)) +
  geom_histogram(
    colour = "black",
    fill = "white",
    na.rm = T,
    size = 0.25,
    stat = 'identity'
  ) +
  xlab("Gender") +
  ylab("Number of Participants")
```

## Procedure

Participants began by reviewing a short set of instructions detailing the experimental design and completing a short quiz verifying their understanding of the task. They then completed `r max(dtl$total_trial)` trials of the list-routines task---`r max(dtl$block)+1` blocks of `r max(dtl$block_trial, na.rm=T)` trials each. The experiment concluded with a brief demographical survey.

In each trial, participants were shown an input list and asked to predict the output according to the rule they thought the computer was using to transform inputs into outputs. Each block used a different rule for transforming inputs into outputs. The `r max(dtl$block)+1` rules were selected randomly from a pool of `r length(unique(dtl$id))` rules being tested, and their order was also randomized. The paradigm encouraged online learning: for trial N+1 of a block, participants saw the correct inputs and outputs from the previous N trials and were able to use them (and their past thinking about the function) to inform their prediction. At the conclusion of each block, participants were asked to describe in words the rule they thought the computer had been using.

Because each participant completed `r max(dtl$block)+1` blocks, we collected data from about `r round(mean((dtl %>% filter(block_trial == 1) %>% group_by(id) %>% dplyr::summarize(n = n()))$n))` participants for each rule.

Participants were paid a flat fee of \$7.50 for participating plus an additional \$0.01 for each correct response, with a median compensation of \$`r round(median(dp$bonus+7.5), 2)` for a median `r round(median(dp$total_time), 1)`min of work. Participants found the task difficult but engaging (Figure \@ref(fig:procedure); Table \@ref(tab:procedure-table)).

```{r procedure, echo=FALSE, warning=FALSE, fig.width=3, fig.height=3, out.width = "24.9%", fig.align='center', fig.show='hold', fig.cap="Histograms of participant ratings of difficulty and engagement, respectively."}
ggplot(dp, aes(x=bonus+7.5)) +
  geom_histogram(binwidth=0.02, colour="black", fill="white", size=0.25, na.rm=T) +
  xlab("Compensation ($)") +
  ylab("Number of Participants")
ggplot(dp, aes(x=total_time)) +
  geom_histogram(binwidth=2, colour="black", fill="white", size=0.25, na.rm=T) +
  xlab("Time-to-Completion (min)") +
  ylab("Number of Participants")
ggplot(dp, aes(x=difficulty)) +
  geom_histogram(binwidth=1, colour="black", fill="white", size=0.25, na.rm=T) +
  xlab("Difficulty (1-7)") +
  ylab("Number of Participants")
ggplot(dp, aes(x=engagement)) +
  geom_histogram(binwidth=1, colour="black", fill="white", size=0.25, na.rm=T) +
  xlab("Engagement (1-7)") +
  ylab("Number of Participants")
```

```{r procedure-table, echo=F, warning=F, result='asis'}
knitr::kable(
  map2_dfr(
    list(
      "Compensation ($)",
      "Time-to-completion (min)",
      "Difficulty (1--7)",
      "Engagement (1--7)"
    ),
    list(
      dp$bonus+7.5,
      dp$total_time,
      dp$difficulty,
      dp$engagement
    ),
    report_spread
  ) %>% select(-w, -p),
  digits = c(NA, 2, 2, 2, 2, 2, 2),
  col.names = c(
    "Demographic",
    "Min",
    "25%",
    "Median",
    "75%",
    "Max",
    "# NA"
  ),
  caption = "A statistical summary of participant demographics.",
  escape=F
) %>% kable_styling()
```

## Materials

Table \@ref(tab:list-functions) describes each function.

```{r list-functions, echo=F, results = "asis"}
tbl <- concepts %>%
  mutate(concept = str_replace(concept, '(.*)', '`\\1`'),
         id = str_replace(id, '(.*)', '`\\1`')) %>%
  select(-pct_learned) %>%
  select(name, gloss, id, purpose, concept) %>%
  pivot_longer(!starts_with("name"),
               names_to = "feature",
               values_to = "Description") %>%
  select(-feature) %>%
  rename(Name = name)
  tbl$Description <- kableExtra::cell_spec(tbl$Description,
                                           extra_css = "display: block; overflow: scroll; height: 1.4em; width: 50em;")
kableExtra::kable_styling(
  knitr::kable(tbl, caption = "The tested list routines from most learned to least learned. Each row gives a memorable name, a verbal description, its unique id, its intended purpose (model-comparison or dataset), and an implementation.", escape = FALSE)
) %>% collapse_rows(columns = 1, valign = "top")
```

# Results

## Task difficulty: overall

```{r learnability, cache=T, echo=F, warning=F}
learnability <- dtl %>%
  group_by(subject, purpose, name) %>%
  dplyr::summarize(learned = !is.na(first_m_of_n(3, 3, accuracy))) %>%
  ungroup()
learnability_by_concept <- learnability %>%
  group_by(name, purpose) %>%
  tidyboot_mean(learned)
learnability_by_purpose <- learnability %>%
  group_by(purpose) %>%
  tidyboot_mean(learned)
```

```{r plot-learnability, dependson="learnability", echo=F, warning=F, fig.align='center', fig.show='hold', out.width="100%", fig.height=5, fig.width=15, fig.cap="(Left): Percentage of people (y-axis) who learned each concept (x-axis). Participants are considered to have learned a concept if they can provide three correct answers in a row; (Right): Mean per-concept learnability (y-axis) by purpose (x-axis). Bars are bootstrapped 95% CIs."}
plot_learnability_by_concept <- function(data) {
    ggplot(data, aes(x = name, y = empirical_stat * 100, fill = purpose)) +
    geom_bar(stat = 'identity') +
    geom_errorbar(
      aes(
        x = name,
        ymin = 100 * ci_lower,
        ymax = 100 * ci_upper
      ),
      position = position_dodge(width = 0.9),
      width = 0.5
    ) +
    xlab("Concept") +
    ylab("Participants Who Learned Concept (%)") +
    coord_cartesian(ylim=c(0,100)) +
    scale_y_continuous(breaks = c(10, 30, 50, 70, 90)) +
    theme(legend.position = "top") +
    theme(axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1
    ))
}
plot_learnability_by_purpose <- function(data) {
  ggplot(data, aes(x = purpose, y = 100 * empirical_stat, fill=purpose)) +
    geom_bar(position = position_dodge(width = 0.9),
             stat = 'identity') +
    geom_errorbar(
      aes(
        x = purpose,
        ymin = 100 * ci_lower,
        ymax = 100 * ci_upper
      ),
      position = position_dodge(width = 0.9),
      width = 0.5
    ) +
    ylab("Participants Who Learned Concept (%)") +
    xlab("Purpose") +
    coord_cartesian(ylim=c(0,100)) +
    theme(legend.position = "none")
}
plot_learnability_by_concept(learnability_by_concept) +
  plot_learnability_by_purpose(learnability_by_purpose) +
  plot_layout(widths = c(2, 1))
```

```{r accuracy, cache=T, echo=F, warning=F}
accuracy <- dtl %>%
  group_by(subject, purpose, name) %>%
  dplyr::summarize(mu = mean(accuracy)) %>%
  ungroup() %>%
  mutate(name = fct_reorder(name, mu, .fun=mean, .desc=T)) %>%
  ungroup()
accuracy_by_concept <- accuracy %>%
  group_by(name, purpose) %>%
  tidyboot_mean(mu)
accuracy_by_purpose <- accuracy %>%
  group_by(purpose) %>%
  tidyboot_mean(mu)
```

```{r plot-accuracy, dependson="accuracy", echo=F, warning=F, fig.align='center', fig.show='hold', out.width="100%", fig.height=5, fig.width=15, fig.cap="(Left): Mean accuracy (y-axis) on each concept (x-axis); (Right): Mean per-concept accuracy (y-axis) by purpose (x-axis). Bars are bootstrapped 95% CIs."}
plot_accuracy_by_concept <- function(data) {
  ggplot(data, aes(x = name, y = empirical_stat * 100, fill = purpose)) +
    geom_bar(stat = 'identity') +
    geom_errorbar(
      aes(
        x = name,
        ymin = 100 * ci_lower,
        ymax = 100 * ci_upper
      ),
      position = position_dodge(width = 0.9),
      width = 0.5
    ) +
    xlab("Concept") +
    ylab("Mean Performance (%)") +
    scale_y_continuous(breaks = c(10, 30, 50, 70, 90)) +
    coord_cartesian(ylim=c(0,100)) +
    theme(legend.position = "top") +
    theme(axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1
    ))
}
plot_accuracy_by_purpose <- function(data) {
  ggplot(data, aes(x = purpose, y = empirical_stat * 100, fill=purpose)) +
    geom_bar(position = position_dodge(width = 0.9),
             stat = 'identity') +
    geom_errorbar(
      aes(
        x = purpose,
        ymin = 100 * ci_lower,
        ymax = 100 * ci_upper
      ),
      position = position_dodge(width = 0.9),
      width = 0.5
    ) +
    ylab("Mean Performance (%)") +
    xlab("Purpose") +
    coord_cartesian(ylim=c(0,100)) +
    theme(legend.position = "none")
}
plot_accuracy_by_concept(accuracy_by_concept) +
  plot_accuracy_by_purpose(accuracy_by_purpose) +
  plot_layout(widths = c(2, 1))
```

```{r how-many, cache=T, echo=F, warning=F}
make_how_many <- function(tbl) {
  total <- length(unique(tbl$subject))
  tbl %>%
    group_by(subject, id) %>%
    dplyr::summarize(learned = !is.na(first_m_of_n(3, 3, accuracy))) %>%
    group_by(subject) %>%
    dplyr::summarize(n_learned = sum(learned)) %>%
    group_by(n_learned) %>%
    dplyr::summarize(n = n()/total)
}
how_many <- make_how_many(dtl)
```

```{r plot-how-many, dependson=c("how-many"), echo=F, warning=F, fig.align='center', fig.show='hold', out.width="66%", fig.height=5, fig.width=10, fig.cap="Number of people (y-axis) who learned N concepts (x-axis)."}
plot_how_many <- function(data) {
  ggplot(data, aes(x=n_learned, y=n*100)) +
    geom_bar(stat='identity', color="black", fill="white") +
    xlab("Number of Functions Learned") +
    ylab("Percentage of Participants (%)") +
    scale_x_continuous(breaks=seq(0,10)) +
    coord_cartesian(xlim=c(0, 10)) +
    theme(legend.position = "none")
}
plot_how_many(how_many)
```

Several of our predictions focused on task difficulty. We predicted that:
1. For most model-comparison tasks, most participants will learn that tasks.
2. Dataset task learnability would be significantly lower than model-comparison task learnability
3. For most dataset tasks, a significant number of participants will learn that task.

I also grouped---post-hoc but without consulting the results---the specific functions involved in the pilot as follows, from easiest to hardest:

1. `constant`, `slice_2_4`, `replace_2_8`, `repeat_head`, `drop_ends`, `98_xs_37`
2. `slice_in_tail`
3. `insert_by_head`, `unique_>_20_desc`, `group_and_drop`, `div_2_or_5`
4. `idx_head_mod_length`, `idx_head_in_tail_sm`, `idx_head_in_tail_lg`
5. `drop_2_or_4`, `append_3_or_9`, `num_repetitions`, `length_in_tail`, `rm_odd_idxs_>_20`, `mod_7_unique`

Group 1 uses the same simple rule every time, without conditions or reasoning about values in the list. Group 2 requires reasoning about the value of items in the list. Group 3 chains multiple operations together, with `div_2_or_5` perhaps being the hardest (you have to figure out what condition separates dividing by 2 from dividing by 5). Group 4 boils a list down to a single number, so the amount of information received on each trial is relatively low. That number, however, is in the list, so the structure of the list can provide hints about the rule. Group 5 either boils down to a single number that isn't in the list or involves complex conditions.

Figure \@ref(fig:plot-learnability).left shows that 7 of the 10 model-comparison functions were learned by most participants. There is only one model-comparison task where fewer than 10\% of participants learned the function This confirms our first hypothesis, though without looking at the specific functions used in the pilot, I would have expected to see another 1--2 functions learned by most people. The figure also shows that: only 1 of the 10 dataset functions was learned by most participants, confirming our second hypothesis; and 6 dataset functions were learned by ~30\% of participants or more, confirming our third hypothesis. That said, there are 4 dataset tasks where fewer of 10\% of participants learned the function, including three that were never learned.

Figure \@ref(fig:plot-learnability).left also tells us the relative difficulty of each concept. There seem to be roughly four groups:

1. easy: `slice_2_4`, `constant`, `98_xs_37`, `idx_head_in_tail_sm`, `drop_ends`, `replace_2_8`, and `repeat_head`
2. moderately easy: `idx_head_in_tail_lg`, `group_and_drop`, `num_repetition`, `slice_in_tail`, `unique_>_20_desc`, and `insert_by_head`
3. moderately_hard: `drop_2_or_4`, and `length_in_tail`
4. hard: `idx_head_mod_length`, `append_3_or_9`, `div_2_or_5`, `mod_7_unique`, and `rm_odd_idxs_>_20`

Compared to my predictions: `idx_head_in_tail_sm` and `num_repetitions` are much easier; `idx_head_in_tail_lg` is slightly easier; `slice_in_tail` and `idx_head_mod_length` are moderately harder; and `div_2_or_5` is much harder.

Speculating on why we see these deviations from my predictions:

- `div_2_or_5`: the comments suggest that people never realized that sometimes you divide by 2 and sometimes by 5. This could be because integer division is an odd concept for people, something they learned but not high in their prior. Also, the eveness/oddness of the length of the list might be an odd condition to consider. In any case, no one articulated the two cases clearly, and no one articulated the condition deciding between cases. 
- `idx_head_mod_length`: the comments suggest that people knew it had to do with placement in the list based on the head, but they didn't know what to do for large heads. Using the least significant digit was a common hypothesis, but cycling around the list or computing the remainder/modulus was not mentioned. Division-based concepts appear difficult.
- `slice_in_tail`: my hypothesis is that this one fails because the examples are bad (e.g. `[8, 32, 9, 54] -> [9, 54]` removes no slice, because the slice doesn't start until index 9, and the endpoint is much higher than 15. 8 of 11 trials are like this.). This is a dataset task, so we could regenerate these trials with additional constraints.
- `num_repetitions`: the concept of the number of repetitions was simply more obvious to people than I expected.
- `idx_head_in_tail_sm` and `idx_head_in_tail_lg`: in 7/11 and 4/11 trials, respectively, you could just guess the last number and be right. This is another case of bad data, but these are model-comparison tasks. In reality, just 1--3 people appear to have acquired the general rule for each of these.

Figure \@ref(fig:plot-learnability).right compares per-function learnability between dataset and model-comparison tasks, further confirming that model-comparison tasks are significantly easier.

Figure \@ref(fig:plot-accuracy) repeats these analyses using per-concept accuracy rather than learnability. The same general trends hold, though the exact ordering of specific functions differs slightly, and the division into four groups perhaps fades into three, with the two moderate groups combining into one.

Finally, Figure \@ref(fig:plot-how-many) shows that most participants learn most of the concepts. 60\% are learning 5 or more concepts, and while none learned all the concepts presented, only 3 learned fewer than 3 concepts. 

## Task difficulty: individual differences

```{r correct-total, cache=T, echo=F, warning=F}
correct_total <- dtl %>%
  group_by(subject, name) %>%
  dplyr::summarize(k = sum(accuracy)) %>%
  mutate(k = factor(x = k, c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))) %>%
  ungroup() %>%
  group_by(name, k, .drop = F) %>%
  dplyr::summarize(n = n())
```

```{r plot-correct-total, dependson=c("correct-total", "correct-per-trial"), echo=F, warning=F, out.width="100%", fig.height=5, fig.width=20, fig.show='hold', fig.align='center', fig.cap="Number of subjects (y-axis) for each possible number of correct responses (x-axis) by concept (colored curves)."}
plot_correct_total <- function(data) {
  ggplot(data, aes(
    x = k,
    y = n,
    group = name,
    fill = name
  )) +
    geom_bar(na.rm = T, stat = 'identity') +
    facet_wrap( ~ name, nrow = 2) +
    xlab("Number of Correct Trials") +
    ylab("Number of Participants") +
    theme(legend.position = "none")
}
plot_correct_total(correct_total)
```

```{r plot-when-learned, echo=F, warning=F, out.width="100%", fig.height=5, fig.width=20, fig.align='center', fig.cap="Number of participants (y-axis) first giving 3 consecutive correct responses on a given trial (x-axis) by concept (subplots). Fill in floating bar shows percentage of subjects who learned concept."}
plot_three <- function(tbl) {
    raw <- tbl %>%
      group_by(subject, name) %>%
      dplyr::summarize(three = first_m_of_n(3,3,accuracy)) %>%
      mutate(three = addNA(factor(three, levels=seq(1, 11)))) %>%
      group_by(name) %>%
      count(three, .drop=F) %>%
      mutate(pct = n/sum(n),
             three = fct_recode(three, NULL="NA")) %>%
      drop_na(three)
    sums <- raw %>% group_by(name) %>% dplyr::summarize(n = sum(pct))
    ggplot(raw, aes(x=three, y=n, group=name, fill=name)) +
      geom_bar(stat="identity") +
      geom_segment(size = 4, color = "#666666",
                   aes(x = 10.5, xend = 10.5, y = 10, yend = 20)) +
      geom_segment(data = sums, size = 4,
                   aes(x = 10.5, xend = 10.5, y = 10, yend = 10+10*n, color=name)) +
      facet_wrap(~name, nrow=2) +
      xlab("First trial to get three in a row") +
      ylab("Number of Participants") +
      scale_y_continuous(breaks=seq(0, 100, by = 20)) +
      theme(legend.position= "none")
}
plot_three(dtl)
```

Figure \@ref(fig:plot-correct-total) shows the distribution over the number of correct trials per participant for each concept. For easy and difficult concepts, the distributions over number of correct trials are fairly tightly clustered---most people get roughly the same number of trials. For moderately difficult concepts, however, the distribution is dispersed. People appear to master the concept at different rates, with some never doing so. **Learning curves which average over all participants may be relatively informative for easy and hard concepts but are probably not the best way to see what's happening for intermediate concepts**.

Figure \@ref(fig:plot-when-learned) plots if/when participants learned each concept, showing high variation in how quickly each concept was learned. Even for the easiest concepts, there's a wide range of times as to when people are judged as having learned the concept. Participant performance isn't uniform, even when the result is, i.e. learning the concept.

```{r correct-per-trial, cache=T, echo=F, warning=F}
correct_per_trial <- dtl %>%
  group_by(name, block_trial) %>%
  tidyboot_mean(column = accuracy, nboot = 500)
```

```{r plot-correct-per-trial, dependson=c("correct-total", "correct-per-trial"), echo=F, warning=F, out.width="100%", fig.height=5, fig.width=20, fig.show='hold', fig.align='center', fig.cap="Mean performance (y-axis) by trial (x-axis) by concept (colored curves), with bootstrapped 95% CI (shaded regions)."}
plot_correct_per_trial <- function(data) {
  ggplot(data, aes(x = block_trial, y = empirical_stat)) +
    geom_ribbon(
      aes(
        x = block_trial,
        ymin = ci_lower,
        ymax = ci_upper,
        fill = name
      ),
      linetype = 0,
      alpha = 0.2
    ) +
    geom_line(aes(color = name), size = 1.5) +
    facet_wrap( ~ name, nrow = 2) +
    xlab("Trial") +
    ylab("Mean Performance (%)") +
    scale_x_continuous(breaks = seq(1, 11)) +
    labs(fill = "Function", color = "Function") +
    theme(legend.position = "none")
}
# plot_correct_per_trial(correct_per_trial)
```


## Learning: not just 3 in a row

```{r learning_fs, cache=T, include=F, echo=F, warning=F}
after_fx <- function(tbl, f, x, pername) {
  data <- tbl %>%
    group_by(subject, name) %>%
    filter(block_trial > f(x,accuracy)) %>%
    ungroup()
  if (pername) {
    data <- data %>% select(name, accuracy) %>% group_by(name)
  } else {
    data <- data %>% select(accuracy)
  }
  data %>%  
    tidyboot_mean(column = accuracy, nboot=500) %>%
    mutate(x = x)
}
make_after_f <- function(tbl, f, pername) {
  seq(0, 11) %>%
    map(~after_fx(tbl, f, ., pername)) %>%
    bind_rows()
}
plot_after_f <- function(data, xlab, pername) {
  plt <- ggplot(data, aes(x=x, y=empirical_stat)) +
    xlab(xlab) +
    ylab("Mean performance") +
    scale_x_continuous(breaks=seq(0,11)) +
    coord_cartesian(xlim=c(0, 11)) +
    theme(legend.position= "none")
  if (pername) {
    plt + 
      geom_bar(aes(group=name, fill=name), width=0.9, stat="identity") +
      geom_errorbar(aes(group=name, ymin = ci_lower, ymax=ci_upper), color="black", size=0.25, width=0.5) +
      facet_wrap(~name, nrow=2)
  } else {
    plt +
      geom_bar(width=0.9, stat="identity") +
      geom_errorbar(aes(ymin = ci_lower, ymax=ci_upper), color="black", size=0.25, width=0.5)
  }
}
plot_backward <- function(data, pername) {
 plt <- ggplot(data, aes(x=timeline, y=empirical_stat)) +
  geom_segment(size=0.25, color="#666666", aes(x = -3.5, xend = 0.5, y=-0.03, yend=-0.03)) +
  xlab("Trial (relative to learning concept)") +
  ylab("Mean Performance") +
  theme(legend.position = "none")
  if (pername) {
    plt +
      geom_bar(aes(group=name, fill=name, alpha=((1-leadup)+0.2)/(2/3)), width=0.8, stat="identity") +
      geom_errorbar(aes(group=name, ymin = ci_lower, ymax = ci_upper), size=0.25, width = 0.5) +
      facet_wrap(~name, nrow=2)
  } else {
    plt +
      geom_bar(aes(alpha=((1-leadup)+0.2)/(2/3)), width=0.8, stat="identity") +
      geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), size=0.25, width = 0.5)
  }
}

```

```{r after-n, cache=T, dependson="learning-fs", include=F, echo=F, warning=F}
after_n <- make_after_f(dtl, first_m, T)
```

```{r plot-after-n, dependson=c("after-n", "learning-fs"), echo=F, warning=F, fig.align='center', out.width="100%", fig.height=5, fig.width=20, fig.cap="Probability of correct response (y-axis) after N correct responses (x-axis) by concept (subplots). Errors bars are bootstrapped 95% CI."}
plot_after_f(after_n, "N correct", T)
```

```{r after-ninarow, cache=T, dependson="learning-fs", include=F, echo=F, warning=F}
after_ninarow <- make_after_f(dtl, function(n,xs) {first_m_of_n(n,n,xs)}, T)
```

```{r plot-after-ninarow, dependson=c("after-ninarow", "learning-fs"), echo=F, warning=F, fig.align='center', out.width="100%", fig.height=5, fig.width=20, fig.cap="Probability of correct response (y-axis) after giving N consecutive correct responses (x-axis) by concept (subplots). Errors bars are bootstrapped 95% CI."}
plot_after_f(after_ninarow, "N in a row correct", T)
```

We assess function learning as correctly responding to three trials in a row. Figures \@ref(fig:plot-after-n)--\@ref(fig:plot-after-ninarow) show that we likely need a better way of identifying when participants learn each concept. 3-in-a-row appears to work well for easy and some moderate concepts, but for other moderate concepts and for difficult concepts, it appears less predictive. It might be the case for several of the harder concepts that learners acquire a portion of the concept that allows them to correctly respond to some trials but not others.

```{r after-n-group, dependson="learning-fs", cache=T, include=F, echo=F, warning=F}
after_n_overall <- make_after_f(dtl, first_m, F)
after_n_drop_7 <- make_after_f(dtl %>% filter(as.integer(name) > 7), first_m, F)
```

```{r plot-after-n-group, dependson=c("after-n-group", "learning-fs"), echo=F, warning=F, fig.show="hold", fig.width="100%", fig.height=5, fig.width=10, fig.align='center', fig.cap="Similar to \\@ref(fig:plot-after-n). (Left): aggregating over all concepts; (Right): aggregating over all but the top 7 concepts."}
plot_after_f(after_n_overall, "N correct", F) + 
  plot_after_f(after_n_drop_7, "N correct", F)
```

```{r after-ninarow-group, cache=T, dependson="learning-fs", include=F, echo=F, warning=F}
after_ninarow_overall <-
  make_after_f(dtl, function(n, xs) {
    first_m_of_n(n, n, xs)
  }, F)
after_ninarow_drop_7 <-
  make_after_f(dtl %>% filter(as.integer(name) > 7), function(n, xs) {
    first_m_of_n(n, n, xs)
  }, F)
```

```{r plot-after-ninarow-group, dependson=c("after-ninarow-group", "learning-fs"), echo=F, warning=F, fig.show="hold", out.width="100%", fig.height=5, fig.width=10, fig.align='center', fig.cap="Similar to \\@ref(fig:plot-after-ninarow). (Left): aggregating over all concepts; (Right): aggregating over all but the top 7 concepts."}
plot_after_f(after_ninarow_overall, "N in a row correct", F) +
  plot_after_f(after_ninarow_drop_7, "N in a row correct", F)
```

We have relatively little data for large numbers of correct trials on the difficult concepts, so to make the point more clearly, we aggregate over concepts. Figures \@ref(fig:plot-after-n-group)--\@ref(fig:plot-after-ninarow-group) show that using either 3 correct or 3-in-a-row correct appears like a good strategy when aggregating over all concepts (left subplots) but looks much less convincing after filtering out the easiest concepts (right subplots).

Figures \@ref(fig:plot-after-n)--\@ref(fig:plot-after-ninarow-group) also show that there's very little difference between getting N overall and getting N-in-a-row. Neither is a particularly good metric for having learned a concept given the limited number of trials available.

```{r backward, cache=T, dependson="learning-fs", echo=F, include=F, warning=F}
backward <- dtl %>%
  group_by(subject, name) %>%
  mutate(timeline = block_trial - first_m_of_n(3,3,accuracy)) %>%
  filter(!is.na(timeline)) %>%
  group_by(name, timeline, .drop=F) %>%
  tidyboot_mean(column = accuracy, nboot=500) %>%
  mutate(leadup = timeline %in% seq(-3, 0))
```

```{r plot-backward, dependson=c("backward", "learning-fs"), echo=F, warning=F, out.width="100%", fig.height=5, fig.width=20, fig.align='center', fig.cap="Backward learning curves in the style of Bower & Trabasso. Mean performance (y-axis) for each trial relative to the first third consecutive correct response (x-axis) by concept (subplots). Error bars are bootstrapped 95% CIs. The underlined portion is constant across all plots (the leadup to three-in-a-row) and so deemphasized for clarity."}
plot_backward(backward, T)
```

```{r backward-overall, cache=T, echo=F, include=F, warning=F}
backward_overall <- dtl %>%
  group_by(subject, name) %>%
  mutate(timeline = block_trial - first_m_of_n(3,3,accuracy)) %>%
  filter(!is.na(timeline)) %>%
  group_by(timeline, .drop=F) %>%
  tidyboot_mean(accuracy, nboot=500) %>%
  mutate(leadup = timeline %in% seq(-3, 0))
backward_drop_7 <- dtl %>%
  filter(as.integer(name) > 7) %>%
  group_by(subject, name) %>%
  mutate(timeline = block_trial - first_m_of_n(3,3,accuracy)) %>%
  filter(!is.na(timeline)) %>%
  group_by(timeline, .drop=F) %>%
  tidyboot_mean(accuracy, nboot=500) %>%
  mutate(leadup = timeline %in% seq(-3, 0))
```

```{r plot-backward-group, dependson=c("backward-overall", "learning-fs"), echo=F, warning=F, out.width="100%", fig.height=5, fig.width=10, fig.align='center', fig.show="hold", fig.cap="Backward learning curves in the style of Bower & Trabasso similar to Figure \\@ref(fig:plot-backward). (Left): aggregating over all concepts; (Right): aggregating over all but the top seven concepts (i.e. dropping the easy concepts)."}
plot_backward(backward_overall, F) + plot_backward(backward_drop_7, F)
```

We can repeat the analysis using a backward learning curve, but there's not enough data/concept to draw clear conclusions (Figure \@ref(fig:plot-backward). Collapsing over concepts in Figure \@ref(fig:plot-backward-group) reinforces the earlier plots in this section: there's certainly a difference in performance after getting 3-in-a-row, but that doesn't imply that performance will be at ceiling.

## Time per block

```{r times-table, echo=FALSE, warning=FALSE, result='asis'}
dm2 <-
  dm %>% mutate(time = time / 60000) %>% pivot_wider(names_from = block, values_from = time)
knitr::kable(
  map2_dfr(
    list(
      "Instructions",
      "Quiz",
      "1",
      "2",
      "3",
      "4",
      "5",
      "6",
      "7",
      "8",
      "9",
      "10",
      "Survey"
    ),
    list(
      dm2$Instructions,
      dm2$Quiz,
      dm2$`1`,
      dm2$`2`,
      dm2$`3`,
      dm2$`4`,
      dm2$`5`,
      dm2$`6`,
      dm2$`7`,
      dm2$`8`,
      dm2$`9`,
      dm2$`10`,
      dm2$Survey
    ),
    report_spread
  ) %>% select(-w,-p,-missing),
  digits = c(NA, 2, 2, 2, 2, 2),
  full_width = F,
  position = "center",
  col.names = c("Block", "Min", "25%", "Median", "75%", "Max"),
  caption = "A statistical summary of time-to-completion by block. All times reported in minutes (min)."
) %>%
  kable_styling()
```

```{r plot-times2, echo=F, warning=F, out.width="100%", fig.height=5, fig.width=30, fig.align='center', fig.show='hold', fig.cap="Deviations from the median block time (y-axis) by block (x-axis) by participant (subplots). Most participants have $\\ge$ 1 substantially longer block."}
plot_times2 <- function(data) {
  data <- data %>% filter(!(block %in% c("Instructions", "Quiz", "Survey")))
  med_block = median(data$time/60000)
  ggplot(data, aes(
    x = block,
    y = (time/60000)/med_block,
  )) +
    geom_point() +
    geom_hline(color="#666666", yintercept=1) +
    facet_wrap(~subject, nrow=1) +
    ylab("Multiple of median block time") +
    xlab("Block") +
    scale_y_continuous(trans="log2") +
    theme(legend.position = "none")
}
plot_times2(dm)
```

We predicted that subjects would on average take 45min to complete the experiment, taking 4min to review the instructions and pass the quiz, just over 5min for the first block, just under 4min per subsequent block, and 1min for the closing survey. Table \@ref(tab:times-table) shows that people were slightly slower on the first block but notably faster on subsequent blocks, such that the sum of the median time per block is less than our initial estimate. The median total time, however, is higher than our initial estimate; Figure \@ref(fig:plot-times2) shows that most participants had at least one block which took them substantially longer than average.

## Participant verbal responses {.tabset}

Below are participant responses to the query at the end of each block, grouped by function and sorted by participant performance (best first), as well as participants comments about the HIT and their programming experience. Notably, while a few participants had significant computer science backgrounds, most reported having no meaningful programming experience.

```{r responses, echo=FALSE, results='asis'}
list_responses_helper <- function(key, data) {
  cat(paste0("\n### ", key, " {-}\n"))
  cat(paste0("`", key[[1]], "`: `", unique(data$concept), "`"))
  print(knitr::kable(data %>% select(Response)) %>% kable_styling())
  NULL
}
list_responses <- function(dt) {
  scores <- dtl %>%
    group_by(subject, name) %>%
    dplyr::summarize(mean.accuracy = mean(accuracy))
  dtv %>%
    left_join(scores, by=c("name", "subject")) %>%
    group_by(name) %>%
    arrange(desc(mean.accuracy)) %>%
    mutate(Response = str_replace_all(response, "\n", " ")) %>%
    group_walk(~list_responses_helper(.y, .x))
  NULL
}
list_responses(dt)
```

### Final Comments {-}
```{r, echo=FALSE, results = "asis"}
list_comments <- function(dp) {
  data <- dp %>%
    drop_na(comments) %>%
    mutate(comments = str_trim(comments),
           len = str_length(comments)) %>%
    arrange(desc(len)) %>%
    filter(!(comments %in% c("none", "n/a", "no", "", "None"))) %>%
    mutate(comments = str_replace_all(comments, "\n", " ")) %>%
    select(workerId, comments) %>%
    rename(Subject = workerId, Response = comments) %>%
    select(Response)
  knitr::kable(data) %>% kable_styling()
}
list_comments(dp)
```

### Experience {-}
```{r, echo=FALSE, results = "asis"}
list_experience <- function(dp) {
  data <- dp %>%
    drop_na(experience) %>%
    mutate(experience = str_trim(experience),
           len = str_length(experience)) %>%
    arrange(desc(len)) %>%
    filter(!(experience %in% c("n/a", ""))) %>%
    mutate(experience = str_replace_all(experience, "\n", " ")) %>%
    select(workerId, experience) %>%
    rename(Subject = workerId, Response = experience) %>%
    select(Response)
  knitr::kable(data) %>% kable_styling()
}
list_experience(dp)
```

# Discussion

## Task difficulty

**While the model-comparison tasks do appear to be easier than the dataset tasks, both are more difficult than I expected**. I expected the model-comparison tasks to cluster at ceiling, and the dataset tasks to smoothly ascend in difficulty. Part of the gap between expectations and reality is my failure to think through all the tasks in the model-comparison problem set (some are relatively difficult, which is, in hindsight, a positive feature for comparing patterns of behavior against humans), but **some tasks may be genuinely harder for participants than I expected**.

Perhaps one reason for this is that **we may have too few trials per task**. Several of the tasks in both problem sets require conditional reasoning. In my experience, it's difficult to tease apart multiple conditions until you get a few examples of each, though that brings you basically to the end of the block for that concept. For these cocnepts, we may be testing too few trials, but adding trials for the easy concepts is likely to bore participants and would increase cost.

Another reason may be that **we're using poor machine-generated trials**. Most machine-generated trials are fine, but some are really uninformative. As a result, there are some concepts where you burn through a significant fraction of the trials without getting much relevant information. I'd like to revisit this in the future and put together a model of how we generate informative trials, but I don't have a great solution given our time constraints, particularly given that we've now run several of the model-comparison models on a significant subset of these trials.

## Learnability

**Using 3-in-a-row as our metric for learning a concept may not be predictive for more difficult concepts**. We see several cases where subjects get 4 or 5 trials right and still have relatively poor performance. 

We want to know whether a person has acquired a concept and if so, on which trial they appear to have acquired it. This metric would ideally apply equally well at the beginning or end of a set of trials as at the middle.

I don't have a particular metric in mind as I write, so I'd appreciate suggestions. The basic idea I keep returning to is that we want an estimate of when performance permanently passes some threshold. One difficulty is that we just don't have that many trials for each concept. Another difficulty is that each trial contributes just a 1 or 0. Together, these make it hard to reliably estimate, e.g., the point at which participants start getting 90% of trials correct. If someone is learning rapidly, it will be difficult to say whether they learned the concept on trial 2 or trial 4. Was the correct response on 2 a fluke, or the result of partial understanding? What about the incorrect result on trial 3? How does our judgment change if they happen to fat-finger trial 7?

We could use the string edit distance between the response and correct output to entrich our measure of accuracy. If, e.g., the function is to remove everything less than 20 and the input is [7 98 15 26 28 3 41 12], then the correct output is [98 26 28 41]. As a string, this is "98 26 28 41". We convert to a string because these are the relevant characters that must be typed in. Someone could fat finger somewhere along the line, and we want to give them partial credit. We could use the same mechanism I use in the TRS model to give partial credit. We'd want to find some way to rescale based on the score of some important lists like the correct answer, the empty list, and the input.

## Generalizability

**I initially had no concerns that these results would generalize, but I'm now concerned that the results are limited by the earlier discussions of difficulty and learnability**.

That said, I selected the concepts to be tested uniformly at random, and tested ~8\% of the available concepts. If we also include results from the first pilot, that takes us up to just over 10\% of the available concepts, and the broad patterns of learnability and accuracy seem similar: there are some easy concepts, some hard concepts, and some that are in between. I think that will hold generally, but I'm not sure we're in a position yet where the specific patterns of difficulty we see are due to the concept as opposed to having too little relevant information.

## Adaptive data collection

**Collecting data adaptively**---collecting additional data on concepts which an initial set of 10 participants suggest are empirically moderately difficult---**still looks like a viable option**. Several of the plots above would benefit from additional data for moderately difficult concepts.

Rather than set the bounds statically at 10% < learnability < 90%, **it might make sense to target a certain sized error bar** on learnability. Consider for example the difference between `slice_2_4` and `repeat_head` in \@ref(fig:plot-learnability).left. Tight error bars might be the most appropriate signal to drive our decision about when we've collected enough data.

## Compensation

On the whole, **our compensation is reasonable**.

We're paying a flat rate of \$7.50 + \$0.01 per correct response. A median compensation of \$`r round(median(dp$bonus+7.5), 2)` for a median `r round(median(dp$total_time), 1)`min of work suggests we're paying approximately \$`r round(median(dp$bonus+7.5)/median(dp$total_time/60), 2)`/hr. Even though this figure includes bonuses, it is lower than our target compensation of \$10/hr, because participants took slightly longer to complete the experiment than we'd anticipated. To compensate, we'd need to pay roughtly an additional \$0.75/participant. The only participant who commented on compensation, however, said, "I found the hit challenging and fun and the pay is decent." People didn't seem upset by insufficient compensation.

# Recommendations to consider

- Add additional trials (and rerun all model-comparison simulations?).
- Manually review dataset trials and regenerate as necessary.
- Manually review model-comparison trials, regenerate as necessary, and ask modelers to rerun.
- Redefine learnability.
- Adopt adaptive sampling based on learnability error bar size.
- Maintain current levels of compensation, all else being equal.
- Extend analysis to look for order effects and estimate task difficulty \& subject performance.
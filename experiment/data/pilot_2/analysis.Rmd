---
title: "Pilot 2 Analysis"
author: "Josh Rule"
date: "6/23/2020"
output:
  bookdown::html_document2:
    keep_md: true
    fig_caption: true
    toc: true
    toc_float:
      collapsed: true
---

```{r setup, include=FALSE}
library(furrr)
library(ggplot2)
library(Hmisc)
library(latex2exp)
library(lme4)
library(lmerTest)
library(lubridate)
library(patchwork)
library(kableExtra)
library(readr)
library(RColorBrewer)
library(reticulate)
library(tidyboot)
library(tidyverse)
library(zoo)

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(eval.after = "fig.cap")
use_virtualenv("~/.data/.virtualenvs/experiment-_gdhu0nT")
options(dplyr.summarise.inform=F)
```

```{r utilities, include=FALSE}
report_spread <- function(title, xs) {
  ## test for normality with Shapiro-Wilk (~shapiro.test~)
  ## if normal, report mean and variance and test results
  ## else report quartiles and test results
  sw <- shapiro.test(xs)
  qs <- quantile(xs, na.rm=T)
  nna <- sum(is.na(xs))
  ## return median, IQR, and failed normality test
  list(title=title, min=qs[1][1], fst=qs[2], median=qs[3], trd=qs[4], max=qs[5], w=sw$statistic, p=sw$p.value, missing=nna)
}
rank_participants <- function(dt) {
  dt %>%
    group_by(subject) %>%
    dplyr::summarize(k.test=sum(accuracy)) %>%
    mutate(rank=rank(1/k.test, ties.method='random')) %>%
    select(-k.test)
}
last_error <- function(xs) {
  # return the index of the last element to contain a 0
  indices <- which(!as.logical(xs))
  if (length(indices) == 0) {
    0
  } else {
    max(indices)
  }
}
m_of_n_correct <- function(m, n, xs) {
  rollapply(xs, n, sum, align="right", fill=0) >= m
}
first_m_of_n <- function(m, n, xs) {
  if (m == 0) {
    0
  } else {
    indices <- which(m_of_n_correct(m, n ,xs))
    if (length(indices) == 0) {
      NA
    } else {
      min(indices)
    }    
  }
}
first_m <- function(m, xs) {
  if (m == 0) {
    0
  } else {
    indices <- which(as.logical(xs))
    if (length(indices) < m) {
      NA
    } else {
      indices[m]
    }
  }
}
incremental_probability <- function(xs) {
  seq_along(xs) %>% map_dbl(~ sum(xs[.:length(xs)]/(length(xs)-.+1)))
}
cumulative_probability <- function(xs) {
  seq_along(xs) %>% map_dbl(~ sum(xs[1:.]/.))
}
exp_weighted_helper <- function(xs) {
  ws <- rev(1/(2^(seq_along(xs)-1)))
  sum(xs*ws)/sum(ws)
}
exp_weighted_probability <- function(xs) {
  seq_along(xs) %>% map_dbl(~ exp_weighted_helper(xs[1:.]))
}
import_time_data <- function(time_path) {
  read_csv(time_path, col_names = T, na = c("", "NA")) %>%
    rename(subject = uniqueid) %>%
    mutate(block = factor(block, levels=c("instructions", "prequiz", "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "postquiz"), labels=c("Instructions", "Quiz", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "Survey")))
}
import_participant_data <- function(participant_path) {
  read_csv(participant_path,
           col_names = T,
           na = c("", "NA", "no-response")) %>%
    select(-starts_with("prequiz")) %>%
    select(-one_of(c("assignmentId", "browser", "mode",
                     "currenttrial", "hitId", "ipadress",
                     "platform", "useragent", "status",
                     "condition", "counterbalance"))) %>%
    rename(
      subject = uniqueid,
      experience = `postquiz-q5`,
      comments = `postquiz-q6`,
      difficulty = `postquiz-q1`,
      engagement = `postquiz-q2`,
      gender = `postquiz-q3`,
      dob = `postquiz-q4`) %>%
    mutate(
      gender = factor(gender),
      dob = as_date(dob),
      dob = replace(dob, dob > ymd("2019-02-05") | dob < ymd("1900-01-01"), NA),
      age = (dob %--% ymd("2020-02-05")) / years(1),
      total_time = (beginhit %--% endhit) / minutes(1))
}
unique_id <- function(purpose, id) {
  str_c(str_sub(purpose, 1, 1), str_sub(id, 2))
}
import_trial_data <- function(trials_path, stimuli_path) {
  read_csv(trials_path, col_names = T) %>%
    select(-phase, -condition, -id) %>%
    rename(subject = uniqueid) %>%
    left_join(read_csv(stimuli_path, col_names = T) %>%
                mutate(id = unique_id(purpose, id)) %>%
                rename(concept = program)) %>%
    mutate(
      id = factor(id),
      block_trial = block_trial + 1,
      log_block_trial = log(block_trial)
    ) %>%
    left_join(rank_participants(.)) %>%
    select(
      subject,
      total_trial,
      block,
      block_trial,
      log_block_trial,
      id,
      accuracy,
      everything()
    )
}
```

```{r dp, include = FALSE}
dp <- import_participant_data("./participants.csv")
```

```{r dm, include = FALSE}
dm <- import_time_data("./times.csv")
```

```{r dt, include = FALSE}
dt <- import_trial_data('./trials.csv', './stimuli.csv')
dtl <- dt %>%
  mutate(id = unique_id(purpose, id)) %>%
  filter(task == '[(i,o)]->i->o') %>%
  group_by(subject, id) %>%
  mutate(mu = mean(accuracy)) %>%
  ungroup()
concepts <- read_csv("./concepts.csv") %>%
  mutate(id = unique_id(purpose, id)) %>%
  left_join(dtl %>% group_by(id) %>% dplyr::summarize(mu = mean(mu))) %>%
  mutate(name = fct_reorder(factor(name), mu, .desc=T))
dtl <- dtl %>% left_join(concepts %>% rename(mu_mu = mu))
dtv <-
  dt %>% filter(task == '[(i,o)]->spec') %>% select(
    -total_trial,
    -block_trial,
    -log_block_trial,
    -id,
    -accuracy,
    -purpose,
    -input,
    -output,
    -rt,
    -order
  ) %>% left_join(concepts)
priors <- bind_rows(read_csv("./model_comparison_priors.csv") %>% mutate(purpose = "model"),
                    read_csv("./dataset_priors.csv") %>% mutate(purpose = "dataset"))%>%
  mutate(id = unique_id(purpose, id)) %>%
  rename(concept = program)
concepts <- concepts %>% left_join(priors)
```

# Introduction 

This document analyzes data collected during a 2020-06-22 pilot of `v0.0.5` of the list-routines experiment. The analysis focuses on questions relevant to running a full-scale study.

# Recommendations

- Regenerate concepts:
  - Generate new concepts for any concept predicted to have < 10\% (20\%?) mean accuracy, including any paired concepts if coming from the model-comparison dataset.
- Regenerate training data:
  - Assess example quality for dataset concepts
  - Pick the best of the 5 tested orderings for training sets scored "reorder", according to my assessment of informativeness for human learners.
  - Update example constraints and generate new i/o pairs for training sets scored "resample".
- Ask modelers to rerun any new training sets for the model-comparison problems.
- Adopt adaptive sampling based on some measure of performance variance.
- Maintain current levels of compensation, all else being equal.

# Method

## Participants

`r length(unique(dp$subject))` people participated in the experiment, hosted on Amazon Mechanical Turk via PsiTurk. Participant age ranged from `r round(min(dp$age), 1)`yrs to `r round(max(dp$age), 1)`yrs (median: `r round(median(dp$age), 1)`yrs), with `r sum(dp$gender == "male", na.rm=T)` males and `r sum(dp$gender == "female", na.rm=T)` females (Figure \@ref(fig:participants)).

```{r participants, echo=FALSE, warning=FALSE, fig.width=3, fig.height=3, out.width = "24.9%", fig.align='center', fig.show='hold', fig.cap="Histograms of participant age and gender, respectively."}
ggplot(dp, aes(x = age)) +
  geom_histogram(
    binwidth = 5,
    colour = "black",
    fill = "white",
    size = 0.25,
    na.rm = T
  ) +
  xlab("Age (yr)") +
  ylab("Number of Participants")

ggplot(dp %>% mutate(gender = fct_explicit_na(gender, na_level = "No Response")) %>% count(gender),
       aes(x = gender, y = n)) +
  geom_histogram(
    colour = "black",
    fill = "white",
    na.rm = T,
    size = 0.25,
    stat = 'identity'
  ) +
  xlab("Gender") +
  ylab("Number of Participants")
```

## Procedure

Participants began by reviewing a short set of instructions detailing the experimental design and completing a short quiz verifying their understanding of the task. They then completed `r max(dtl$total_trial)` trials of the list-routines task---`r max(dtl$block)+1` blocks of `r max(dtl$block_trial, na.rm=T)` trials each. The experiment concluded with a brief demographical survey.

In each trial, participants were shown an input list and asked to predict the output according to the rule they thought the computer was using to transform inputs into outputs. Each block used a different rule for transforming inputs into outputs. The `r max(dtl$block)+1` rules were selected randomly from a pool of `r length(unique(dtl$id))` rules being tested, and their order was also randomized. The paradigm encouraged online learning: for trial N+1 of a block, participants saw the correct inputs and outputs from the previous N trials and were able to use them (and their past thinking about the function) to inform their prediction. At the conclusion of each block, participants were asked to describe in words the rule they thought the computer had been using.

Because each participant completed `r max(dtl$block)+1` blocks, we collected data from about `r round(mean((dtl %>% filter(block_trial == 1) %>% group_by(id) %>% dplyr::summarize(n = n()))$n))` participants for each rule.

Participants were paid a flat fee of \$7.50 for participating plus an additional \$0.01 for each correct response, with a median compensation of \$`r round(median(dp$bonus+7.5), 2)` for a median `r round(median(dp$total_time), 1)`min of work. Participants found the task difficult but engaging (Figure \@ref(fig:procedure); Table \@ref(tab:procedure-table)).

```{r procedure, echo=FALSE, warning=FALSE, fig.width=3, fig.height=3, out.width = "24.9%", fig.align='center', fig.show='hold', fig.cap="Histograms of participant ratings of difficulty and engagement, respectively."}
ggplot(dp, aes(x=bonus+7.5)) +
  geom_histogram(binwidth=0.02, colour="black", fill="white", size=0.25, na.rm=T) +
  xlab("Compensation ($)") +
  ylab("Number of Participants")
ggplot(dp, aes(x=total_time)) +
  geom_histogram(binwidth=2, colour="black", fill="white", size=0.25, na.rm=T) +
  xlab("Time-to-Completion (min)") +
  ylab("Number of Participants")
ggplot(dp, aes(x=difficulty)) +
  geom_histogram(binwidth=1, colour="black", fill="white", size=0.25, na.rm=T) +
  xlab("Difficulty (1-7)") +
  ylab("Number of Participants")
ggplot(dp, aes(x=engagement)) +
  geom_histogram(binwidth=1, colour="black", fill="white", size=0.25, na.rm=T) +
  xlab("Engagement (1-7)") +
  ylab("Number of Participants")
```

```{r procedure-table, echo=F, warning=F, result='asis'}
knitr::kable(
  map2_dfr(
    list(
      "Compensation ($)",
      "Time-to-completion (min)",
      "Difficulty (1--7)",
      "Engagement (1--7)"
    ),
    list(
      dp$bonus+7.5,
      dp$total_time,
      dp$difficulty,
      dp$engagement
    ),
    report_spread
  ) %>% select(-w, -p),
  digits = c(NA, 2, 2, 2, 2, 2, 2),
  col.names = c(
    "Demographic",
    "Min",
    "25%",
    "Median",
    "75%",
    "Max",
    "# NA"
  ),
  caption = "A statistical summary of participant demographics.",
  escape=F
) %>% kable_styling()
```

## Materials

Table \@ref(tab:list-functions) describes each function.

```{r list-functions, echo=F, results = "asis"}
tbl <- concepts %>%
  mutate(concept = str_replace(concept, '(.*)', '`\\1`'),
         id = str_replace(id, '(.*)', '`\\1`')) %>%
  select(name, gloss, id, purpose, concept) %>%
  pivot_longer(!starts_with("name"),
               names_to = "feature",
               values_to = "Description") %>%
  select(-feature) %>%
  rename(Name = name)
  tbl$Description <- kableExtra::cell_spec(tbl$Description,
                                           extra_css = "display: block; overflow: scroll; height: 1.4em; width: 50em;")
  knitr::kable(tbl, caption = "The tested list routines from most learned to least learned. Each row gives a memorable name, a verbal description, its unique id, its intended purpose (model-comparison or dataset), and an implementation.", escape = FALSE) %>%
    kable_styling() %>%
    collapse_rows(columns = 1, valign = "top")
```

# Results

## Task difficulty in the pilot

```{r accuracy, cache=T, echo=F, warning=F}
accuracy_by_concept <- dtl %>%
  group_by(subject, name, purpose, mu, mu_mu) %>%
  group_keys() %>%
  group_by(name, purpose) %>%
  tidyboot_mean(mu)
accuracy_by_purpose <- dtl %>%
  group_by(subject, name, purpose, mu, mu_mu) %>%
  group_keys() %>%
  group_by(purpose) %>%
  tidyboot_mean(mu)
```

```{r plot-accuracy, dependson="accuracy", echo=F, warning=F, fig.align='center', fig.show='hold', out.width="100%", fig.height=5, fig.width=15, fig.cap="(Left): Mean accuracy (y-axis) on each concept (x-axis); (Right): Mean per-concept accuracy (y-axis) by purpose (x-axis). Bars are bootstrapped 95% CIs."}
plot_accuracy_by_concept <- function(data) {
  ggplot(data, aes(x = name, y = empirical_stat * 100, fill = purpose)) +
    geom_bar(stat = 'identity') +
    geom_errorbar(
      aes(
        x = name,
        ymin = 100 * ci_lower,
        ymax = 100 * ci_upper
      ),
      position = position_dodge(width = 0.9),
      width = 0.5
    ) +
    xlab("Concept") +
    ylab("Mean Performance (%)") +
    scale_y_continuous(breaks = c(10, 30, 50, 70, 90)) +
    coord_cartesian(ylim=c(0,100)) +
    theme(legend.position = "top") +
    theme(axis.text.x = element_text(
      angle = 90,
      vjust = 0.5,
      hjust = 1
    ))
}
plot_accuracy_by_purpose <- function(data) {
  ggplot(data, aes(x = purpose, y = empirical_stat * 100, fill=purpose)) +
    geom_bar(position = position_dodge(width = 0.9),
             stat = 'identity') +
    geom_errorbar(
      aes(
        x = purpose,
        ymin = 100 * ci_lower,
        ymax = 100 * ci_upper
      ),
      position = position_dodge(width = 0.9),
      width = 0.5
    ) +
    ylab("Mean Performance (%)") +
    xlab("Purpose") +
    coord_cartesian(ylim=c(0,100)) +
    theme(legend.position = "none")
}
plot_accuracy_by_concept(accuracy_by_concept) +
  plot_accuracy_by_purpose(accuracy_by_purpose) +
  plot_layout(widths = c(2, 1))
```

Several of our predictions focused on task difficulty. We predicted that:

1. For most model-comparison tasks, most participants will learn that tasks.
2. Dataset task learnability would be significantly lower than model-comparison task learnability
3. For most dataset tasks, a significant number of participants will learn that task.

I also grouped---post-hoc but without consulting the results---the specific functions involved in the pilot as follows, from easiest to hardest:

1. `constant`, `slice_2_4`, `replace_2_8`, `repeat_head`, `drop_ends`, `98_xs_37`
2. `slice_in_tail`
3. `insert_by_head`, `unique_>_20_desc`, `group_and_drop`, `div_2_or_5`
4. `idx_head_mod_length`, `idx_head_in_tail_sm`, `idx_head_in_tail_lg`
5. `drop_2_or_4`, `append_3_or_9`, `num_repetitions`, `length_in_tail`, `rm_odd_idxs_>_20`, `mod_7_unique`

Group 1 uses the same simple rule every time, without conditions or reasoning about values in the list. Group 2 requires reasoning about the value of items in the list. Group 3 chains multiple operations together, with `div_2_or_5` perhaps being the hardest (you have to figure out what condition separates dividing by 2 from dividing by 5). Group 4 boils a list down to a single number, so the amount of information received on each trial is relatively low. That number, however, is in the list, so the structure of the list can provide hints about the rule. Group 5 either boils down to a single number that isn't in the list or involves complex conditions.

Figure \@ref(fig:plot-accuracy).left shows that 7 of the 10 model-comparison functions were learned by most participants. There is only one model-comparison task where fewer than 10\% of participants learned the function This confirms our first hypothesis, though without looking at the specific functions used in the pilot, I would have expected to see another 1--2 functions learned by most people. The figure also shows that: only 1 of the 10 dataset functions was learned by most participants, confirming our second hypothesis; and 6 dataset functions were learned by ~30\% of participants or more, confirming our third hypothesis. That said, there are 4 dataset tasks where fewer of 10\% of participants learned the function, including three that were never learned.

Figure \@ref(fig:plot-accuracy).left also tells us the relative difficulty of each concept. There seem to be roughly four groups:

1. easy: `slice_2_4`, `constant`, `98_xs_37`, `drop_ends`, `replace_2_8`, and `repeat_head` 
2. moderately easy: `idx_head_in_tail_sm`, `unique_>_20_desc`,  `drop_2_or_4`, `num_repetition`, `slice_in_tail`, `idx_head_in_tail_lg`, `group_and_drop`,  and `insert_by_head`
3. moderately_hard: `length_in_tail`, `idx_head_mod_length`, `div_2_or_5`, `append_3_or_9`, and `rm_odd_idxs_>_20`
4. hard: `mod_7_unique` (the only concept where accuracy isn't statistically different from 0)

Compared to my predictions: `idx_head_in_tail_sm` and `num_repetitions` are much easier; `idx_head_in_tail_lg` is moderately easier; `slice_in_tail` and `idx_head_mod_length` are moderately harder; and `div_2_or_5` is much harder.

Speculating on why we see these deviations from my predictions:

- `div_2_or_5`: the comments suggest that people never realized that sometimes you divide by 2 and sometimes by 5. This could be because integer division is an odd concept for people, something they learned but not high in their prior. Also, the eveness/oddness of the length of the list might be an odd condition to consider. In any case, no one articulated the two cases clearly, and no one articulated the condition deciding between cases. 
- `idx_head_mod_length`: the comments suggest that people knew it had to do with placement in the list based on the head, but they didn't know what to do for large heads. Using the least significant digit was a common hypothesis, but cycling around the list or computing the remainder/modulus was not mentioned. Division-based concepts appear difficult.
- `slice_in_tail`: my hypothesis is that this one fails because the examples are bad (e.g. `[8, 32, 9, 54] -> [9, 54]` removes no slice, because the slice doesn't start until index 9, and the endpoint is much higher than 15. 8 of 11 trials are like this.). This is a dataset task, so we could regenerate these trials with additional constraints.
- `num_repetitions`: the concept of the number of repetitions was simply more obvious to people than I expected.
- `idx_head_in_tail_sm` and `idx_head_in_tail_lg`: in 7/11 and 4/11 trials, respectively, you could just guess the last number and be right. This is another case of bad data, but these are model-comparison tasks. In reality, just 1--3 people appear to have acquired the general rule for each of these.

Figure \@ref(fig:plot-accuracy).right summarizes per-function accuracy between dataset and model-comparison tasks, confirming that model-comparison tasks are significantly easier.

## Predicting task difficulty

```{r predictive-model, cache=T, echo=F, warning=F}
predictive_model <- function(raw_data) {
data <- raw_data %>% filter(!(name %in% c("group_and_drop", "idx_head_in_tail_sm")))
model <- lm(mu ~ depth + length + visible + semi, data=data)
print(summary(model))
model
}
length_model <- function(raw_data) {
data <- raw_data %>% filter(!(name %in% c("group_and_drop", "idx_head_in_tail_sm")))
model <- lm(mu ~ length, data=data)
print(summary(model))
model
}
length_model <- length_model(concepts)
model <- predictive_model(concepts)
```

```{r plot-predictive-model, dependson="predictive-model", echo=F, warning=F, fig.align='center', out.width="100%", fig.height=5, fig.width=10, fig.cap="Length-based predictive models of difficulty. (Left): Linear regression predictions of mean acuracy based solely on concept length in the DSL; (Right): Linear regression predictions of mean acuracy based on concept length in the DSL, adjusted for the amount of visible and semi-visible information."}
plot_predictive_model <- function(model, length_model, raw_data) {
  data <- raw_data %>% filter(!(name %in% c("group_and_drop", "idx_head_in_tail_sm")))
  length_data <- data %>% mutate(predict = predict(length_model, data))
  data <- data %>% mutate(predict = predict(model, data))
  p1 <- ggplot(length_data, aes(x = mu, y = predict, color=name)) +
    geom_point() +
    geom_abline(intercept=c(0,0), slope=1) +
    xlab("Mean Accuracy") +
    ylab("Model Prediction (length)") +
    coord_cartesian(xlim=c(0,1), ylim=c(0,1))
  p2 <- ggplot(data, aes(x = mu, y = predict, color=name)) +
    geom_point() +
    geom_abline(intercept=c(0,0), slope=1) +
    xlab("Mean Accuracy") +
    ylab("Model Prediction (depth + visible + semi + length)") +
    coord_cartesian(xlim=c(0,1), ylim=c(0,1))
  p1 + p2
}
plot_predictive_model(model, length_model, concepts)
```

```{r plot-predictive-model-predictions, dependson="predictive-model", echo=F, warning=F, fig.align='center', out.width="100%", fig.height=5, fig.width=12, fig.cap="Program-based predictive models of difficulty. Linear regression predictions (y-axis) of mean accuracy (x-axis) based on concept length in the DSL, adjusted for the amount of visible and semi-visible information. Black entries are the actual difficulties of tested concepts."}
plot_predictive_model <- function(model, priors, concepts) {
  data <- priors %>%
    mutate(Tested = factor(concept %in% concepts$concept, labels = c("Untested", "Tested")))
  data <- data %>% mutate(predict = predict(model, data)) %>% left_join(concepts %>% select(id, mu))
  ggplot(data, aes(x = predict, fill = purpose)) +
    geom_histogram(bins=50) +
    geom_histogram(data = data %>% filter(Tested == "Tested"), aes(x=mu), bins=50, fill="black") +
    facet_wrap(~purpose, nrow=2) +
    
    xlab("Model Predicted Mean Accuracy") +
    ylab("Number of concepts")
}
plot_predictive_model(model, priors, concepts)
```

We'd like to be able to predict task difficulty, so we can decide whether our concepts are too easy, too difficult, or sitting in the sweet spot for human learning. Let's start with a simple model based on the prior probability of each concept in the DSL. Over all `r nrow(priors)` problems, negative log prior and length in the DSL are strongly correlated ($\rho =$ `r cor(-priors$prior, priors$length, use="complete.obs")`. Length is also much easier to compute and in the same units as some of the other data I've collected about visibility, so we'll use length in what follows. Figure \@ref(fig:plot-predictive-model).left shows the relationship between mean accuracy and length over our pilot concepts. The correlation is terrible because of some notable outliters. We can improve it by adjusting for two additional kinds of information about each program: its depth, and the amount of program structure visible in the final output. Figure \@ref(fig:plot-predictive-model).right shows the impact of these changes on the model's predictiveness.

We can apply this improved model to the 230 untested concepts to get some indication for how difficult they are likely to be. Figure \@ref(fig:plot-predictive-model-predictions) plots these predictions and suggests that while we have some extremely difficult that are probably worth redefining, we have a pretty nice distribution over concept difficulty. If anything, we may want to make those extremely difficult concepts fairly moderate to fill in the distribution. If, for example, we decide to replace anything with a predicted accuracy below 10\%, Table \@ref(tab:worst-predicted-accuracies) shows what needs to be replaced.

```{r worst-predicted-accuracies, echo=F, warning=F, result="asis"}
worst_predicted_accuracies <- function(tbl) {
  data <- tbl %>% mutate(predict = predict(model, tbl), concept = str_replace(concept, '(.*)', '`\\1`')) %>% select(id, predict, concept) %>% arrange(predict) %>% filter(predict < 0.1) %>% mutate(predict = round(predict, 3))
  data$concept <- kableExtra::cell_spec(data$concept,
                                           extra_css = "display: block; overflow: scroll; height: 1.4em; width: 50em;")
  kable(data, escape=F, col.names=c("Id", "Accuracy", "Program"), caption="Predicted accuracies (Accuracy) for the concepts with  predicted accuracy < 10%. Note that only 4 are from the model-comparison (i.e. id starts with \"m\").") %>% kable_styling()
}
worst_predicted_accuracies(priors)
```

**Note**: We wondered how many categories we needed for dealing with the visibility of a node in the program, where by visibility we mean how directly its impact can be seen in a given input/output pair. The model coefficients for `visible` and `semi` (two non-zero levels of visiblity), however, are pretty similar. This could mean that I wasn't consistent in assigning nodes to these two categories, that these categories aren't that different from one another, or that we need a more fine-grained method for assigning visibility scores to nodes.

**NOTE**: I exclude `idx_head_in_tail_sm` and `group_and_drop` from these analyses because I think the data more strongly reflects extremely bad examples than the difficulty of the concepts.

## Example quality

```{r model-comparison-examples, include=F, echo=F, warning=F}
mc_examples <- read_csv("./model_comparison_examples.csv")
```

I haven't yet analyzed the example quality for the dataset concepts.

I have done analyzed the model-comparison concepts, assigning each to 1 of 3 categories: "good" (examples are good), "order" (examples are okay so long as we change the order), or "resample" (examples need to be resampled). We've run the models on 5 orderings of the same trials, so "order" just suggests selecting a better trial ordering from these 5 for the human experiment---we won't need to rerun the models.

Of the `r nrow(mc_examples)` concepts, `r nrow(mc_examples %>% filter(example == 1))` scored "good", `r nrow(mc_examples %>% filter(example == 2))` scored "order", and `r nrow(mc_examples %>% filter(example == 3))` scored "resample". For those scoring "resample", Table \@ref(tab:model-comparison-examples-predicted-accuracies) shows the predicted accuracies. For the four concepts with the lowest predicted accuracies, it might make sense to generate new concepts altogether.

```{r model-comparison-examples-predicted-accuracies, echo=F, warning=F, result="asis"}
mc_examples_predicted_accuracies <- function(tbl) {
  data <- tbl %>% mutate(purpose = "model", id = unique_id(purpose, id)) %>% filter(example == 3) %>% left_join(priors, by=c("id", "purpose"))
  data <- data %>% mutate(predict = round(predict(model, data), 3), concept = str_replace(concept, '(.*)', '`\\1`')) %>% select(id, predict, concept) %>% arrange(desc(predict))
  data$concept <- kableExtra::cell_spec(data$concept,
                                           extra_css = "display: block; overflow: scroll; height: 1.4em; width: 50em;")
  kable(data, escape=F, col.names=c("Id", "Predicted Accuracy", "Program"), caption="Predicted accuracies (Accuracy) for the model-comparison concepts (Id; Concept) I've assessed as needing new training data.") %>% kable_styling()
}
mc_examples_predicted_accuracies(mc_examples)
```

## Learning is not just 3-in-a-row

```{r learning_fs, cache=T, include=F, echo=F, warning=F}
after_fx <- function(tbl, f, x, pername) {
  data <- tbl %>%
    group_by(subject, name) %>%
    filter(block_trial > f(x,accuracy)) %>%
    ungroup()
  if (pername) {
    data <- data %>% select(name, accuracy) %>% group_by(name)
  } else {
    data <- data %>% select(accuracy)
  }
  data %>%  
    tidyboot_mean(column = accuracy, nboot=500) %>%
    mutate(x = x)
}
make_after_f <- function(tbl, f, pername) {
  seq(0, 11) %>%
    map(~after_fx(tbl, f, ., pername)) %>%
    bind_rows()
}
plot_after_f <- function(data, xlab, pername) {
  plt <- ggplot(data, aes(x=x, y=empirical_stat)) +
    xlab(xlab) +
    ylab("Mean performance") +
    scale_x_continuous(breaks=seq(0,11)) +
    coord_cartesian(xlim=c(0, 11)) +
    theme(legend.position= "none")
  if (pername) {
    plt + 
      geom_bar(aes(group=name, fill=name), width=0.9, stat="identity") +
      geom_errorbar(aes(group=name, ymin = ci_lower, ymax=ci_upper), color="black", size=0.25, width=0.5) +
      facet_wrap(~name, nrow=2)
  } else {
    plt +
      geom_bar(width=0.9, stat="identity") +
      geom_errorbar(aes(ymin = ci_lower, ymax=ci_upper), color="black", size=0.25, width=0.5)
  }
}
plot_backward <- function(data, pername) {
 plt <- ggplot(data, aes(x=timeline, y=empirical_stat)) +
  geom_segment(size=0.25, color="#666666", aes(x = -3.5, xend = 0.5, y=-0.03, yend=-0.03)) +
  xlab("Trial (relative to learning concept)") +
  ylab("Mean Performance") +
  theme(legend.position = "none")
  if (pername) {
    plt +
      geom_bar(aes(group=name, fill=name, alpha=((1-leadup)+0.2)/(2/3)), width=0.8, stat="identity") +
      geom_errorbar(aes(group=name, ymin = ci_lower, ymax = ci_upper), size=0.25, width = 0.5) +
      facet_wrap(~name, nrow=2)
  } else {
    plt +
      geom_bar(aes(alpha=((1-leadup)+0.2)/(2/3)), width=0.8, stat="identity") +
      geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), size=0.25, width = 0.5)
  }
}

```

```{r after-n, cache=T, dependson="learning-fs", include=F, echo=F, warning=F}
after_n <- make_after_f(dtl, first_m, T)
```

```{r plot-after-n, dependson=c("after-n", "learning-fs"), echo=F, warning=F, fig.align='center', out.width="100%", fig.height=5, fig.width=20, fig.cap="Probability of correct response (y-axis) after N correct responses (x-axis) by concept (subplots). Errors bars are bootstrapped 95% CI."}
plot_after_f(after_n, "N correct", T)
```

```{r after-ninarow, cache=T, dependson="learning-fs", include=F, echo=F, warning=F}
after_ninarow <- make_after_f(dtl, function(n,xs) {first_m_of_n(n,n,xs)}, T)
```

```{r plot-after-ninarow, dependson=c("after-ninarow", "learning-fs"), echo=F, warning=F, fig.align='center', out.width="100%", fig.height=5, fig.width=20, fig.cap="Probability of correct response (y-axis) after giving N consecutive correct responses (x-axis) by concept (subplots). Errors bars are bootstrapped 95% CI."}
plot_after_f(after_ninarow, "N in a row correct", T)
```

We assess function learning as correctly responding to three trials in a row. Figures \@ref(fig:plot-after-n)--\@ref(fig:plot-after-ninarow) show that we likely need a better way of identifying when participants learn each concept. 3-in-a-row appears to work well for easy and some moderate concepts, but for other moderate concepts and for difficult concepts, it appears less predictive. It might be the case for several of the harder concepts that learners acquire a portion of the concept that allows them to correctly respond to some trials but not others.

```{r after-n-group, dependson="learning-fs", cache=T, include=F, echo=F, warning=F}
after_n_overall <- make_after_f(dtl, first_m, F)
after_n_drop_7 <- make_after_f(dtl %>% filter(as.integer(name) > 7), first_m, F)
```

```{r plot-after-n-group, dependson=c("after-n-group", "learning-fs"), echo=F, warning=F, fig.show="hold", fig.width="100%", fig.height=5, fig.width=10, fig.align='center', fig.cap="Similar to \\@ref(fig:plot-after-n). (Left): aggregating over all concepts; (Right): aggregating over all but the top 7 concepts."}
plot_after_f(after_n_overall, "N correct", F) + 
  plot_after_f(after_n_drop_7, "N correct", F)
```

```{r after-ninarow-group, cache=T, dependson="learning-fs", include=F, echo=F, warning=F}
after_ninarow_overall <-
  make_after_f(dtl, function(n, xs) {
    first_m_of_n(n, n, xs)
  }, F)
after_ninarow_drop_7 <-
  make_after_f(dtl %>% filter(as.integer(name) > 7), function(n, xs) {
    first_m_of_n(n, n, xs)
  }, F)
```

```{r plot-after-ninarow-group, dependson=c("after-ninarow-group", "learning-fs"), echo=F, warning=F, fig.show="hold", out.width="100%", fig.height=5, fig.width=10, fig.align='center', fig.cap="Similar to \\@ref(fig:plot-after-ninarow). (Left): aggregating over all concepts; (Right): aggregating over all but the top 7 concepts."}
plot_after_f(after_ninarow_overall, "N in a row correct", F) +
  plot_after_f(after_ninarow_drop_7, "N in a row correct", F)
```

We have relatively little data for large numbers of correct trials on the difficult concepts, so to make the point more clearly, we aggregate over concepts. Figures \@ref(fig:plot-after-n-group)--\@ref(fig:plot-after-ninarow-group) show that using either 3 correct or 3-in-a-row correct appears like a good strategy when aggregating over all concepts (left subplots) but looks much less convincing after filtering out the easiest concepts (right subplots).

Figures \@ref(fig:plot-after-n)--\@ref(fig:plot-after-ninarow-group) also show that there's very little difference between getting N overall and getting N-in-a-row. Neither is a particularly good metric for having learned a concept given the limited number of trials available.

```{r backward, cache=T, dependson="learning-fs", echo=F, include=F, warning=F}
backward <- dtl %>%
  group_by(subject, name) %>%
  mutate(timeline = block_trial - first_m_of_n(3,3,accuracy)) %>%
  filter(!is.na(timeline)) %>%
  group_by(name, timeline, .drop=F) %>%
  tidyboot_mean(column = accuracy, nboot=500) %>%
  mutate(leadup = timeline %in% seq(-3, 0))
```

```{r plot-backward, dependson=c("backward", "learning-fs"), echo=F, warning=F, out.width="100%", fig.height=5, fig.width=20, fig.align='center', fig.cap="Backward learning curves in the style of Bower & Trabasso. Mean performance (y-axis) for each trial relative to the first third consecutive correct response (x-axis) by concept (subplots). Error bars are bootstrapped 95% CIs. The underlined portion is constant across all plots (the leadup to three-in-a-row) and so deemphasized for clarity."}
plot_backward(backward, T)
```

```{r backward-overall, cache=T, echo=F, include=F, warning=F}
backward_overall <- dtl %>%
  group_by(subject, name) %>%
  mutate(timeline = block_trial - first_m_of_n(3,3,accuracy)) %>%
  filter(!is.na(timeline)) %>%
  group_by(timeline, .drop=F) %>%
  tidyboot_mean(accuracy, nboot=500) %>%
  mutate(leadup = timeline %in% seq(-3, 0))
backward_drop_7 <- dtl %>%
  filter(as.integer(name) > 7) %>%
  group_by(subject, name) %>%
  mutate(timeline = block_trial - first_m_of_n(3,3,accuracy)) %>%
  filter(!is.na(timeline)) %>%
  group_by(timeline, .drop=F) %>%
  tidyboot_mean(accuracy, nboot=500) %>%
  mutate(leadup = timeline %in% seq(-3, 0))
```

```{r plot-backward-group, dependson=c("backward-overall", "learning-fs"), echo=F, warning=F, out.width="100%", fig.height=5, fig.width=10, fig.align='center', fig.show="hold", fig.cap="Backward learning curves in the style of Bower & Trabasso similar to Figure \\@ref(fig:plot-backward). (Left): aggregating over all concepts; (Right): aggregating over all but the top seven concepts (i.e. dropping the easy concepts)."}
plot_backward(backward_overall, F) + plot_backward(backward_drop_7, F)
```

We can repeat the analysis using a backward learning curve, but there's not enough data/concept to draw clear conclusions (Figure \@ref(fig:plot-backward). Collapsing over concepts in Figure \@ref(fig:plot-backward-group) reinforces the earlier plots in this section: there's certainly a difference in performance after getting 3-in-a-row, but that doesn't imply that performance will be at ceiling.

## Individual differences

```{r correct-total, cache=T, echo=F, warning=F}
correct_total <- dtl %>%
  group_by(subject, name) %>%
  dplyr::summarize(k = sum(accuracy)) %>%
  mutate(k = factor(x = k, c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))) %>%
  ungroup() %>%
  group_by(name, k, .drop = F) %>%
  dplyr::summarize(n = n())
```

```{r plot-correct-total, dependson=c("correct-total", "correct-per-trial"), echo=F, warning=F, out.width="100%", fig.height=5, fig.width=20, fig.show='hold', fig.align='center', fig.cap="Number of subjects (y-axis) for each possible number of correct responses (x-axis) by concept (colored curves)."}
plot_correct_total <- function(data) {
  ggplot(data, aes(
    x = k,
    y = n,
    group = name,
    fill = name
  )) +
    geom_bar(na.rm = T, stat = 'identity') +
    facet_wrap( ~ name, nrow = 2) +
    xlab("Number of Correct Trials") +
    ylab("Number of Participants") +
    theme(legend.position = "none")
}
plot_correct_total(correct_total)
```

```{r correct-per-trial, cache=T, echo=F, warning=F}
correct_per_trial <- dtl %>%
  group_by(name, purpose, block_trial) %>%
  tidyboot_mean(column = accuracy, nboot = 500)
```

```{r plot-correct-per-trial, dependson=c("correct-total", "correct-per-trial"), echo=F, warning=F, out.width="100%", fig.height=5, fig.width=20, fig.show='hold', fig.align='center', fig.cap="Mean performance (y-axis) by trial (x-axis) by concept (colored curves), with bootstrapped 95% CI (shaded regions)."}
plot_correct_per_trial <- function(data) {
  ggplot(data, aes(x = block_trial, y = empirical_stat)) +
    geom_ribbon(
      aes(
        x = block_trial,
        ymin = ci_lower,
        ymax = ci_upper,
        fill = purpose
      ),
      linetype = 1,
      alpha = 0.2
    ) +
    geom_linerange(
      aes(
        x = block_trial,
        ymin = ci_lower,
        ymax = ci_upper,
        color = purpose
      ),
      linetype = 1,
      alpha = 0.9,
    ) +
    geom_line(aes(color = purpose), size = 1.5) +
    facet_wrap( ~ name, nrow = 2) +
    xlab("Trial") +
    ylab("Mean Performance (%)") +
    scale_x_continuous(breaks = seq(1, 11)) +
    labs(fill = "Function", color = "Function") +
    theme(legend.position = "none")
}
plot_correct_per_trial(correct_per_trial)
```

```{r accuracy_over_time, cache=T, echo=F, warning=F}
accuracy_over_time <- dtl %>%
  group_by(subject, name) %>%
  arrange(block_trial) %>%
  mutate(performance = cumsum(accuracy))
```

```{r plot-accuracy-over-time, dependson=c("accuracy-curves"), echo=F, warning=F, out.width="100%", fig.height=5, fig.width=20, fig.show='hold', fig.align='center', fig.cap="Mean performance (y-axis) by trial (x-axis) by concept (colored curves), with bootstrapped 95% CI (shaded regions)."}
plot_accuracy_over_time <- function(data) {
  ggplot(data, aes(x = block_trial, y = performance, group=subject, color=subject)) +
    geom_line(position=position_jitter(height=0.15, width=0.15), size = 1.5) +
    facet_wrap( ~ name, nrow = 2) +
    xlab("Trial") +
    ylab("Cumulative Correct") +
    scale_x_continuous(breaks = seq(1, 11)) +
    labs(fill = "Function", color = "Function") +
    theme(legend.position = "none")
}
plot_accuracy_over_time(accuracy_over_time)
```

If we don't use 3-in-a-row, we'll need some other metric for deciding when to continue collecting data from additional participants. I don't have the answer, but I think we may want to look at our understanding of inter-participant/intra-concept variance, how many patterns of response do we see within a concept?

Figures \@ref(fig:plot-correct-total)--\@ref(fig:plot-accuracy-over-time) show a few ways of looking at that. For easy and difficult concepts, the variance in these plots is fairly low---most people are doing roughly the same thing. For moderately difficult concepts, however, the distributions are dispersed. People appear to master the concept at different rates, with some never doing so. Learning curves which average over all participants may be relatively informative for easy and hard concepts but are probably not the best way to see what's happening for intermediate concepts. We want a better measure of this variance.

## Time per block

```{r times-table, echo=FALSE, warning=FALSE, result='asis'}
dm2 <-
  dm %>% mutate(time = time / 60000) %>% pivot_wider(names_from = block, values_from = time)
knitr::kable(
  map2_dfr(
    list(
      "Instructions",
      "Quiz",
      "1",
      "2",
      "3",
      "4",
      "5",
      "6",
      "7",
      "8",
      "9",
      "10",
      "Survey"
    ),
    list(
      dm2$Instructions,
      dm2$Quiz,
      dm2$`1`,
      dm2$`2`,
      dm2$`3`,
      dm2$`4`,
      dm2$`5`,
      dm2$`6`,
      dm2$`7`,
      dm2$`8`,
      dm2$`9`,
      dm2$`10`,
      dm2$Survey
    ),
    report_spread
  ) %>% select(-w,-p,-missing),
  digits = c(NA, 2, 2, 2, 2, 2),
  full_width = F,
  position = "center",
  col.names = c("Block", "Min", "25%", "Median", "75%", "Max"),
  caption = "A statistical summary of time-to-completion by block. All times reported in minutes (min)."
) %>%
  kable_styling()
```

```{r plot-times2, echo=F, warning=F, out.width="100%", fig.height=5, fig.width=30, fig.align='center', fig.show='hold', fig.cap="Deviations from the median block time (y-axis) by block (x-axis) by participant (subplots). Most participants have $\\ge$ 1 substantially longer block."}
plot_times2 <- function(data) {
  data <- data %>% filter(!(block %in% c("Instructions", "Quiz", "Survey")))
  med_block = median(data$time/60000)
  ggplot(data, aes(
    x = block,
    y = (time/60000)/med_block,
  )) +
    geom_point() +
    geom_hline(color="#666666", yintercept=1) +
    facet_wrap(~subject, nrow=1) +
    ylab("Multiple of median block time") +
    xlab("Block") +
    scale_y_continuous(trans="log2") +
    theme(legend.position = "none")
}
plot_times2(dm)
```

We predicted that subjects would on average take 45min to complete the experiment, taking 4min to review the instructions and pass the quiz, just over 5min for the first block, just under 4min per subsequent block, and 1min for the closing survey. Table \@ref(tab:times-table) shows that people were slightly slower on the first block but notably faster on subsequent blocks, such that the sum of the median time per block is less than our initial estimate. The median total time, however, is higher than our initial estimate; Figure \@ref(fig:plot-times2) shows that most participants had at least one block which took them substantially longer than average.

## Participant verbal responses {.tabset}

Below are participant responses to the query at the end of each block, grouped by function and sorted by participant performance (best first), as well as participants comments about the HIT and their programming experience. Notably, while a few participants had significant computer science backgrounds, most reported having no meaningful programming experience.

```{r responses, echo=FALSE, results='asis'}
list_responses_helper <- function(key, data) {
  cat(paste0("\n### ", key, " {-}\n"))
  cat(paste0("`", key[[1]], "`: `", unique(data$concept), "`"))
  print(knitr::kable(data %>% select(Response)) %>% kable_styling())
  NULL
}
list_responses <- function(dt) {
  scores <- dtl %>%
    group_by(subject, name) %>%
    dplyr::summarize(mean.accuracy = mean(accuracy))
  dtv %>%
    left_join(scores, by=c("name", "subject")) %>%
    group_by(name) %>%
    arrange(desc(mean.accuracy)) %>%
    mutate(Response = str_replace_all(response, "\n", " ")) %>%
    group_walk(~list_responses_helper(.y, .x))
  NULL
}
list_responses(dt)
```

### Final Comments {-}
```{r, echo=FALSE, results = "asis"}
list_comments <- function(dp) {
  data <- dp %>%
    drop_na(comments) %>%
    mutate(comments = str_trim(comments),
           len = str_length(comments)) %>%
    arrange(desc(len)) %>%
    filter(!(comments %in% c("none", "n/a", "no", "", "None"))) %>%
    mutate(comments = str_replace_all(comments, "\n", " ")) %>%
    select(workerId, comments) %>%
    rename(Subject = workerId, Response = comments) %>%
    select(Response)
  knitr::kable(data) %>% kable_styling()
}
list_comments(dp)
```

### Experience {-}
```{r, echo=FALSE, results = "asis"}
list_experience <- function(dp) {
  data <- dp %>%
    drop_na(experience) %>%
    mutate(experience = str_trim(experience),
           len = str_length(experience)) %>%
    arrange(desc(len)) %>%
    filter(!(experience %in% c("n/a", ""))) %>%
    mutate(experience = str_replace_all(experience, "\n", " ")) %>%
    select(workerId, experience) %>%
    rename(Subject = workerId, Response = experience) %>%
    select(Response)
  knitr::kable(data) %>% kable_styling()
}
list_experience(dp)
```

# Discussion

## Task

**While the model-comparison tasks do appear to be easier than the dataset tasks, both are more difficult than I expected**. I expected the model-comparison tasks to cluster at ceiling, and the dataset tasks to smoothly ascend in difficulty. By contrast, **many tasks appear genuinely harder for participants than I expected**. In hindsight, this is a positive feature for comparing patterns of behavior against humans, but we want to make sure we don't overdo it.

Perhaps one reason for this is that **some tasks are too complex for the number of trials we're running**. Several of the tasks in both problem sets require conditional reasoning. In my experience, it's difficult to tease apart multiple conditions until you get a few examples of each, though that brings you basically to the end of the block for that concept. For these cocnepts, we may be testing too few trials, but adding trials for the easy concepts is likely to bore participants and would increase cost. It will b easier to throw out these concepts and generate easier ones.

Another reason may be that **we're using poor machine-generated trials**. Most machine-generated trials are fine, but some are really uninformative. As a result, there are some concepts where you burn through a significant fraction of the trials without getting much relevant information. I'd like to revisit this in the future and put together a model of how we generate informative trials, but given our time constraints, our best option is probably to generate a few new concepts and training sets with new more informative objectives for what makes for a good example of that concept.

**Given these limitations, I don't want to make any strong claims based on these results.**.

That said, I selected the concepts to be tested uniformly at random, and tested ~8\% of the available concepts. If we also include results from the first pilot, that takes us up to just over 10\% of the available concepts, and the broad patterns of learnability and accuracy seem similar: there are some easy concepts, some hard concepts, and some that are in between. I think that will hold generally, but I'm not sure we're in a position yet where the specific patterns of difficulty we see are due to the concept as opposed to having too little relevant information. If we address the concerns I've identified with hard concepts and bad examples, I think we'll collect much more reliable \& meaningful data.

## Individual Differences and adaptive data collection

**Using 3-in-a-row as a metric for learning a concept doesn't appear predictive for more difficult concepts**.

We're mostly using learnability as a metric for understanding when we need to collect more data: we want enough data to adequately capture variance between participants for a single concept. That is, rather than set the bounds statically at 10% < 3-in-a-row < 90%, **it might make sense to target a certain upper bound on variance in participant responses**. I don't have a particular metric in mind as I write---I'd appreciate suggestions---but the data in \@ref(fig:plot-accuracy-over-time) seems like a good place to start.

Assuming we find a good measure of variance, **collecting data adaptively**---collecting additional data on concepts which an initial set of 10 participants suggest are empirically moderately difficult---**still looks like a viable option**. Several of the plots above would benefit from additional data for moderately difficult concepts.

**NOTE**: We could use the string edit distance between the response and correct output to entrich our measure of accuracy. If, e.g., the function is to remove everything less than 20 and the input is [7 98 15 26 28 3 41 12], then the correct output is [98 26 28 41]. As a string, this is "98 26 28 41". We convert to a string because these are the relevant characters that must be typed in. Someone could fat finger somewhere along the line, and we want to give them partial credit. We could use the same mechanism I use in the TRS model to give partial credit. We'd want to find some way to rescale based on the score of some important lists like the correct answer, the empty list, and the input.

## Compensation

On the whole, **our compensation is reasonable**.

We're paying a flat rate of \$7.50 + \$0.01 per correct response. A median compensation of \$`r round(median(dp$bonus+7.5), 2)` for a median `r round(median(dp$total_time), 1)`min of work suggests we're paying approximately \$`r round(median(dp$bonus+7.5)/median(dp$total_time/60), 2)`/hr. Even though this figure includes bonuses, it is lower than our target compensation of \$10/hr, because participants took slightly longer to complete the experiment than we'd anticipated. To compensate, we'd need to pay roughtly an additional \$0.75/participant. The only participant who commented on compensation, however, said, "I found the hit challenging and fun and the pay is decent." People didn't seem upset by insufficient compensation.
